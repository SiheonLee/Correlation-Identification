{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first time to create venv and install packages\n",
    "\n",
    "# %conda create -n corrid-dev anaconda -y\n",
    "# %conda activate corrid-dev -y\n",
    "\n",
    "# %conda install -n corrid-dev pandas -y\n",
    "\n",
    "# %conda install pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_pre_process: ./logs/data_bpic17_readyToUse_preprocessed_for_adaptation_classification.csv\n",
      "Columns before removal:\n",
      "['ApplicationType', 'LoanGoal', 'RequestedAmount', 'CreditScore', 'timesincefirstcase', 'duration', 'FirstWithdrawalAmount', 'MonthlyCost', 'NumberOfTerms', 'OfferedAmount', 'open_cases', 'month', 'weekday', 'hour', 'treatment']\n",
      "Columns after removal:\n",
      "['ApplicationType', 'LoanGoal', 'RequestedAmount', 'CreditScore', 'timesincefirstcase', 'duration', 'FirstWithdrawalAmount', 'MonthlyCost', 'NumberOfTerms', 'OfferedAmount', 'open_cases', 'month', 'weekday', 'hour', 'treatment']\n",
      "\n",
      "log_pre_process: ./logs/sample_sequence_simulation_logs_multi_adapted_noisy_encoded.csv\n",
      "Columns before removal:\n",
      "['trace_id', 'event:event_name@A', 'event:event_name@B', 'event:event_name@C', 'event:event_name@D', 'event:event_name@E', 'event:event_name@F', 'event:event_name@I', 'event:event_name@process end event', 'event:event_name@process start event', 'event:concept:name@A', 'event:concept:name@B', 'event:concept:name@C', 'event:concept:name@D', 'event:concept:name@E', 'event:concept:name@F', 'event:concept:name@I', 'event:concept:name@process end event', 'event:concept:name@process start event', 'event:adaptation_action@insert', 'event:adaptation_action@no-action', 'event:adaptation_action@skip', 'event:@@index', 'event:@@case_index', 'event:start_weekday', 'event:resource', 'event:duration', 'event:trace_id', 'succession:concept:name@A#B', 'succession:concept:name@B#C', 'succession:concept:name@C#D', 'succession:concept:name@D#E', 'succession:concept:name@D#F', 'succession:concept:name@E#F', 'succession:concept:name@F#process end event', 'succession:concept:name@process end event#I', 'succession:concept:name@process start event#A', 'trace:cycle_time', 'adaptation_action']\n",
      "Columns after removal:\n",
      "['trace_id', 'event:start_weekday', 'event:duration', 'trace:cycle_time', 'adaptation_action']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy\n",
    "import pandas as pd\n",
    "\n",
    "logs_folder = './logs/'\n",
    "\n",
    "def log_pre_process(csv_file_path, memory_reduction, columns_to_drop=[]):\n",
    "    print(f'log_pre_process: {csv_file_path}')\n",
    "    data_df = pd.read_csv(csv_file_path, quotechar=\"'\")\n",
    "\n",
    "    print('Columns before removal:')\n",
    "    list_of_column_names = list(data_df.columns)\n",
    "    print(list_of_column_names)\n",
    "\n",
    "    data_df = data_df.drop(columns=columns_to_drop)\n",
    "\n",
    "    print('Columns after removal:')\n",
    "    list_of_column_names = list(data_df.columns)\n",
    "    print(list_of_column_names)\n",
    "    print()\n",
    "\n",
    "    slice_start_col = 0\n",
    "    slice_end_col = len(data_df.columns)-1\n",
    "\n",
    "    values = data_df.iloc[:, slice_start_col:slice_end_col]\n",
    "    classes = data_df.iloc[:, slice_end_col:]\n",
    "\n",
    "    if memory_reduction:\n",
    "        enc = preprocessing.OrdinalEncoder(dtype=numpy.int8)\n",
    "        values = enc.fit_transform(values)\n",
    "\n",
    "    data = (values, classes)\n",
    "    return data, list_of_column_names[:-1], data_df\n",
    "\n",
    "\n",
    "bpic17_logs_with_interventions_path = logs_folder + 'data_bpic17_readyToUse_preprocessed_for_adaptation_classification.csv'\n",
    "bpic17_logs_columns_to_drop = []\n",
    "bpic17_logs_with_interventions, bpic17_logs_with_interventions_column_names, bpic17_logs_with_interventions_df = log_pre_process(bpic17_logs_with_interventions_path, memory_reduction=True, columns_to_drop=bpic17_logs_columns_to_drop)\n",
    "\n",
    "synthetic_logs_with_adaptations_path = logs_folder + 'sample_sequence_simulation_logs_multi_adapted_noisy_encoded.csv'\n",
    "synthetic_logs_columns_to_drop = [\"event:event_name@A\",\"event:event_name@B\",\"event:event_name@C\",\"event:event_name@D\",\"event:event_name@E\",\"event:event_name@F\",\"event:event_name@I\",\"event:event_name@process end event\",\"event:event_name@process start event\",\"event:concept:name@A\",\"event:concept:name@B\",\"event:concept:name@C\",\"event:concept:name@D\",\"event:concept:name@E\",\"event:concept:name@F\",\"event:concept:name@I\",\"event:concept:name@process end event\",\"event:concept:name@process start event\",\"event:adaptation_action@insert\",\"event:adaptation_action@no-action\",\"event:adaptation_action@skip\",\"event:@@index\",\"event:@@case_index\",\"event:trace_id\",\"succession:concept:name@A#B\",\"succession:concept:name@B#C\",\"succession:concept:name@C#D\",\"succession:concept:name@D#E\",\"succession:concept:name@D#F\",\"succession:concept:name@E#F\",\"succession:concept:name@F#process end event\",\"succession:concept:name@process end event#I\",\"succession:concept:name@process start event#A\", \"event:resource\"]\n",
    "synthetic_logs_with_adaptations, synthetic_logs_with_adaptations_column_names, synthetic_logs_with_adaptations_df = log_pre_process(synthetic_logs_with_adaptations_path, memory_reduction=False, columns_to_drop=synthetic_logs_columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Classifier comparison\n",
    "\n",
    "A comparison of a several classifiers in scikit-learn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------\n",
      "dataset: bpic17_logs_with_interventions\n",
      "------------------------------------------------------\n",
      "Classifier: Decision Tree\n",
      "10-fold cross-validation mean F1: 0.7654498728572802\n",
      "|--- CreditScore <= 39.50\n",
      "|   |--- hour <= 13.50\n",
      "|   |   |--- weekday <= 4.50\n",
      "|   |   |   |--- class: 1\n",
      "|   |   |--- weekday >  4.50\n",
      "|   |   |   |--- class: 0\n",
      "|   |--- hour >  13.50\n",
      "|   |   |--- class: 0\n",
      "|--- CreditScore >  39.50\n",
      "|   |--- CreditScore <= 40.50\n",
      "|   |   |--- weekday <= 4.50\n",
      "|   |   |   |--- hour <= 13.50\n",
      "|   |   |   |   |--- class: 0\n",
      "|   |   |   |--- hour >  13.50\n",
      "|   |   |   |   |--- class: 0\n",
      "|   |   |--- weekday >  4.50\n",
      "|   |   |   |--- class: 0\n",
      "|   |--- CreditScore >  40.50\n",
      "|   |   |--- class: 1\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "dataset: synthetic_logs_with_adaptations\n",
      "------------------------------------------------------\n",
      "Classifier: Decision Tree\n",
      "10-fold cross-validation mean F1: 0.9237324182574183\n",
      "|--- trace:cycle_time <= 250.09\n",
      "|   |--- trace:cycle_time <= 214.36\n",
      "|   |   |--- trace:cycle_time <= 206.70\n",
      "|   |   |   |--- class: no-action\n",
      "|   |   |--- trace:cycle_time >  206.70\n",
      "|   |   |   |--- class: no-action\n",
      "|   |--- trace:cycle_time >  214.36\n",
      "|   |   |--- trace:cycle_time <= 224.70\n",
      "|   |   |   |--- trace:cycle_time <= 224.44\n",
      "|   |   |   |   |--- class: no-action\n",
      "|   |   |   |--- trace:cycle_time >  224.44\n",
      "|   |   |   |   |--- class: insert\n",
      "|   |   |--- trace:cycle_time >  224.70\n",
      "|   |   |   |--- class: no-action\n",
      "|--- trace:cycle_time >  250.09\n",
      "|   |--- class: skip\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.tree import export_text\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "names = [\n",
    "    # \"Nearest Neighbors\",\n",
    "    # \"Linear SVM\",\n",
    "    # \"RBF SVM\",\n",
    "\n",
    "    # \"Gaussian Process\",\n",
    "    \"Decision Tree\",\n",
    "    # \"Random Forest\",\n",
    "    # \"Neural Net\",\n",
    "    # \"AdaBoost\",\n",
    "    # \"Naive Bayes\",\n",
    "    # \"QDA\",\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    # KNeighborsClassifier(3),\n",
    "    # SVC(kernel=\"linear\", C=0.025),\n",
    "    # SVC(gamma=2, C=1),\n",
    "\n",
    "    # GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=4, min_samples_leaf=2, ccp_alpha=0.003),\n",
    "    # RandomForestClassifier(max_depth=6, n_estimators=10, max_features=1),\n",
    "    # MLPClassifier(alpha=1, max_iter=1000),\n",
    "    # AdaBoostClassifier(),\n",
    "    # GaussianNB(),\n",
    "    # QuadraticDiscriminantAnalysis(),\n",
    "]\n",
    "\n",
    "datasets_df = [\n",
    "    bpic17_logs_with_interventions_df,\n",
    "    synthetic_logs_with_adaptations_df\n",
    "]\n",
    "\n",
    "datasets = [\n",
    "    bpic17_logs_with_interventions,\n",
    "    synthetic_logs_with_adaptations\n",
    "]\n",
    "\n",
    "dataset_names = [\n",
    "    \"bpic17_logs_with_interventions\",\n",
    "    \"synthetic_logs_with_adaptations\"\n",
    "]\n",
    "\n",
    "dataset_feature_names = [\n",
    "    bpic17_logs_with_interventions_column_names,\n",
    "    synthetic_logs_with_adaptations_column_names\n",
    "]\n",
    "\n",
    "# iterate over datasets\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    \n",
    "    X, y = ds\n",
    "\n",
    "    print('---------------------------------------------------------------------')\n",
    "    print(f'dataset: {dataset_names[ds_cnt]}')\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "\n",
    "        print('------------------------------------------------------')\n",
    "        print('Classifier:', name)\n",
    "\n",
    "        if name != 'Decision Tree':\n",
    "            # preprocess dataset, normalize features\n",
    "            X = StandardScaler().fit_transform(X)\n",
    "\n",
    "        scores = cross_val_score(clf, X, y.values.ravel(), cv=10, scoring='f1_weighted')\n",
    "        \n",
    "        print('10-fold cross-validation mean F1:', numpy.mean(scores))\n",
    "\n",
    "        if name == 'Decision Tree':\n",
    "            clf.fit(X, y.values.ravel())\n",
    "            text_representation = tree.export_text(clf, feature_names=dataset_feature_names[ds_cnt])\n",
    "            print(text_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi train / test\n",
      "\n",
      " dataset: bpic17_logs_with_interventions\n",
      "train percentage= 20\n",
      "f1 score= 0.7850651108672028\n",
      "train percentage= 40\n",
      "f1 score= 0.7661996207016786\n",
      "train percentage= 66\n",
      "f1 score= 0.7583210230912191\n",
      "\n",
      " dataset: synthetic_logs_with_adaptations\n",
      "train percentage= 20\n",
      "f1 score= 0.8971681415929202\n",
      "train percentage= 40\n",
      "f1 score= 0.9351336055619529\n",
      "train percentage= 66\n",
      "f1 score= 0.9288397552860364\n"
     ]
    }
   ],
   "source": [
    "models_folder = './models/'\n",
    "\n",
    "def store_classifier(classifier, file_path):\n",
    "    from joblib import dump\n",
    "    dump(classifier, models_folder + file_path)\n",
    "\n",
    "def load_classifier(file_path):\n",
    "    from joblib import load\n",
    "    classifier = load(clf, models_folder + file_path)\n",
    "    return classifier\n",
    "\n",
    "def store_logs(dataframe, path):\n",
    "    dataframe.to_csv(logs_folder + path, columns=dataframe.columns, index=False)\n",
    "\n",
    "\n",
    "def multi_train_test(train_percentages, test_train_ratio):\n",
    "    print('Multi train / test')\n",
    "    classifier = DecisionTreeClassifier(max_depth=4, min_samples_leaf=2, ccp_alpha=0.003)\n",
    "\n",
    "    for ds_cnt, dataset in enumerate(datasets):\n",
    "        print(f'\\n dataset: {dataset_names[ds_cnt]}')\n",
    "        values, classes = dataset\n",
    "        # print(values)\n",
    "        # print(classes)\n",
    "        dataset_length = len(dataset[0])\n",
    "\n",
    "        for train_percentage in train_percentages:\n",
    "            print(f'train percentage= {train_percentage}')\n",
    "            split_point_train = int(train_percentage / 100 * dataset_length)\n",
    "            split_point_test = int(split_point_train + test_train_ratio * split_point_train)\n",
    "\n",
    "            train_set_values = values[:split_point_train]\n",
    "            train_set_classes = classes[:split_point_train]\n",
    "\n",
    "            test_set_values = values[split_point_train:split_point_test]\n",
    "            test_set_classes = classes[split_point_train:split_point_test]\n",
    "\n",
    "            classifier.fit(train_set_values, train_set_classes)\n",
    "            y_pred = classifier.predict(test_set_values)\n",
    "            f1 = f1_score(y_true = test_set_classes, y_pred = y_pred, average = 'weighted')\n",
    "            print(f'f1 score= {f1}')\n",
    "            \n",
    "            classifier_file_path = dataset_names[ds_cnt] + '_' + str(train_percentage) + '_percent' + '.joblib'\n",
    "            store_classifier(classifier, classifier_file_path)\n",
    "\n",
    "            train_logs_path = dataset_names[ds_cnt] + '_' + str(train_percentage) + '_percent_train' + '.csv'\n",
    "            store_logs(datasets_df[ds_cnt][:split_point_train], train_logs_path)\n",
    "\n",
    "            test_logs_path = dataset_names[ds_cnt] + '_' + str(train_percentage) + '_percent_test' + '.csv'\n",
    "            store_logs(datasets_df[ds_cnt][split_point_train:split_point_test], test_logs_path)\n",
    "\n",
    "\n",
    "train_percentages = [20, 40, 66]\n",
    "test_train_ratio = 0.5\n",
    "multi_train_test(train_percentages, test_train_ratio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "corrid-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "34ff90f5239f9160625527e13c748f95b5880a9ad27056497ef1c46bfe84f008"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

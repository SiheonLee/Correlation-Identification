{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first time to create venv and install packages\n",
    "\n",
    "# %conda create -n corrid-dev anaconda -y\n",
    "# %conda activate corrid-dev -y\n",
    "\n",
    "# %conda install -n corrid-dev pandas -y\n",
    "\n",
    "# %conda install pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode: data_bpic17_readyToUse_preprocessed_for_adaptation_classification.csv\n",
      "       ApplicationType_Limit raise  ApplicationType_New credit  LoanGoal_Car  \\\n",
      "0                                0                           1             0   \n",
      "1                                0                           1             0   \n",
      "2                                0                           1             0   \n",
      "3                                0                           1             1   \n",
      "4                                0                           1             0   \n",
      "...                            ...                         ...           ...   \n",
      "31408                            0                           1             1   \n",
      "31409                            0                           1             0   \n",
      "31410                            0                           1             0   \n",
      "31411                            0                           1             1   \n",
      "31412                            0                           1             0   \n",
      "\n",
      "       LoanGoal_Caravan / Camper  LoanGoal_Existing loan takeover  \\\n",
      "0                              0                                1   \n",
      "1                              0                                0   \n",
      "2                              0                                0   \n",
      "3                              0                                0   \n",
      "4                              0                                0   \n",
      "...                          ...                              ...   \n",
      "31408                          0                                0   \n",
      "31409                          0                                1   \n",
      "31410                          0                                0   \n",
      "31411                          0                                0   \n",
      "31412                          0                                0   \n",
      "\n",
      "       LoanGoal_Extra spending limit  LoanGoal_Home improvement  \\\n",
      "0                                  0                          0   \n",
      "1                                  0                          1   \n",
      "2                                  0                          1   \n",
      "3                                  0                          0   \n",
      "4                                  0                          1   \n",
      "...                              ...                        ...   \n",
      "31408                              0                          0   \n",
      "31409                              0                          0   \n",
      "31410                              0                          1   \n",
      "31411                              0                          0   \n",
      "31412                              0                          1   \n",
      "\n",
      "       LoanGoal_Motorcycle  LoanGoal_Not speficied  \\\n",
      "0                        0                       0   \n",
      "1                        0                       0   \n",
      "2                        0                       0   \n",
      "3                        0                       0   \n",
      "4                        0                       0   \n",
      "...                    ...                     ...   \n",
      "31408                    0                       0   \n",
      "31409                    0                       0   \n",
      "31410                    0                       0   \n",
      "31411                    0                       0   \n",
      "31412                    0                       0   \n",
      "\n",
      "       LoanGoal_Other see explanation  ...   duration  FirstWithdrawalAmount  \\\n",
      "0                                   0  ...  13.248566                20000.0   \n",
      "1                                   0  ...   6.134470                  500.0   \n",
      "2                                   0  ...  12.819864                15000.0   \n",
      "3                                   0  ...  26.988859                 3726.0   \n",
      "4                                   0  ...  31.750191                35000.0   \n",
      "...                               ...  ...        ...                    ...   \n",
      "31408                               0  ...  22.644617                 5000.0   \n",
      "31409                               0  ...   5.576960                 8304.0   \n",
      "31410                               0  ...  22.625626                10000.0   \n",
      "31411                               0  ...  22.926296                 8304.0   \n",
      "31412                               0  ...  15.509351                20000.0   \n",
      "\n",
      "       MonthlyCost  NumberOfTerms  OfferedAmount  open_cases  month  weekday  \\\n",
      "0           498.29             44        20000.0         815      1        3   \n",
      "1           200.00             33         6000.0         431      1        3   \n",
      "2           158.98            120        15000.0         790      1        2   \n",
      "3           252.73             72        15700.0        1240      1        1   \n",
      "4           366.08            120        35000.0         340      1        2   \n",
      "...            ...            ...            ...         ...    ...      ...   \n",
      "31408        97.40             60         5000.0        1193      1        3   \n",
      "31409       150.00            127        15000.0        1451      1        0   \n",
      "31410       106.46            120        10000.0         697      1        4   \n",
      "31411       450.00             77        30000.0         556      1        1   \n",
      "31412       297.81             77        20000.0        1142      1        4   \n",
      "\n",
      "       hour  treatment  \n",
      "0         9          1  \n",
      "1        11          0  \n",
      "2        12          1  \n",
      "3         8          1  \n",
      "4         9          0  \n",
      "...     ...        ...  \n",
      "31408    11          1  \n",
      "31409    13          0  \n",
      "31410    10          1  \n",
      "31411    14          1  \n",
      "31412     6          0  \n",
      "\n",
      "[31413 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# encode logs (string features with OneHotEncoder)\n",
    "import pandas as pd\n",
    "\n",
    "logs_folder = './logs/'\n",
    "bpic17_logs_with_interventions_path = 'data_bpic17_readyToUse_preprocessed_for_adaptation_classification.csv'\n",
    "bpic17_logs_with_interventions_path_encoded = 'data_bpic17_readyToUse_preprocessed_for_adaptation_classification_encoded.csv'\n",
    "\n",
    "def encode(dataframe):\n",
    "    from sklearn import preprocessing\n",
    "    enc = preprocessing.OneHotEncoder(handle_unknown='ignore', sparse=False, dtype=int)\n",
    "    return enc.fit_transform(dataframe), enc\n",
    "\n",
    "def store_logs(dataframe, path):\n",
    "    dataframe.to_csv(logs_folder + path, columns=dataframe.columns, index=False)\n",
    "\n",
    "def load_logs(path):\n",
    "    return pd.read_csv(logs_folder + path, quotechar=\"'\")\n",
    "\n",
    "print(f'encode: {bpic17_logs_with_interventions_path}')\n",
    "data_df = load_logs(bpic17_logs_with_interventions_path)\n",
    "\n",
    "column_names_to_encode = ['ApplicationType', 'LoanGoal',]\n",
    "columns_to_encode = data_df.loc[:, column_names_to_encode]\n",
    "encoded_columns, encoder = encode(columns_to_encode)\n",
    "encoded_df = pd.DataFrame(encoded_columns, columns=encoder.get_feature_names_out())\n",
    "\n",
    "data_df_without_encoded_columns = data_df.drop(column_names_to_encode, axis='columns')\n",
    "data_encoded_df = encoded_df.join(data_df_without_encoded_columns)\n",
    "# data_encoded_df = data_encoded_df.reset_index()\n",
    "print(data_encoded_df)\n",
    "\n",
    "store_logs(data_encoded_df, bpic17_logs_with_interventions_path_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_pre_process: data_bpic17_readyToUse_preprocessed_for_adaptation_classification_encoded.csv\n",
      "Columns before removal:\n",
      "['ApplicationType_Limit raise', 'ApplicationType_New credit', 'LoanGoal_Car', 'LoanGoal_Caravan / Camper', 'LoanGoal_Existing loan takeover', 'LoanGoal_Extra spending limit', 'LoanGoal_Home improvement', 'LoanGoal_Motorcycle', 'LoanGoal_Not speficied', 'LoanGoal_Other see explanation', 'LoanGoal_Remaining debt home', 'LoanGoal_Unknown', 'LoanGoal_other', 'RequestedAmount', 'CreditScore', 'timesincefirstcase', 'duration', 'FirstWithdrawalAmount', 'MonthlyCost', 'NumberOfTerms', 'OfferedAmount', 'open_cases', 'month', 'weekday', 'hour', 'treatment']\n",
      "Columns after removal:\n",
      "['ApplicationType_Limit raise', 'ApplicationType_New credit', 'LoanGoal_Car', 'LoanGoal_Caravan / Camper', 'LoanGoal_Existing loan takeover', 'LoanGoal_Extra spending limit', 'LoanGoal_Home improvement', 'LoanGoal_Motorcycle', 'LoanGoal_Not speficied', 'LoanGoal_Other see explanation', 'LoanGoal_Remaining debt home', 'LoanGoal_Unknown', 'LoanGoal_other', 'RequestedAmount', 'CreditScore', 'timesincefirstcase', 'duration', 'FirstWithdrawalAmount', 'MonthlyCost', 'NumberOfTerms', 'OfferedAmount', 'open_cases', 'month', 'weekday', 'hour', 'treatment']\n",
      "\n",
      "log_pre_process: sample_sequence_simulation_logs_multi_adapted_noisy_encoded.csv\n",
      "Columns before removal:\n",
      "['trace_id', 'event:event_name@A', 'event:event_name@B', 'event:event_name@C', 'event:event_name@D', 'event:event_name@E', 'event:event_name@F', 'event:event_name@I', 'event:event_name@process end event', 'event:event_name@process start event', 'event:concept:name@A', 'event:concept:name@B', 'event:concept:name@C', 'event:concept:name@D', 'event:concept:name@E', 'event:concept:name@F', 'event:concept:name@I', 'event:concept:name@process end event', 'event:concept:name@process start event', 'event:adaptation_action@insert', 'event:adaptation_action@no-action', 'event:adaptation_action@skip', 'event:@@index', 'event:@@case_index', 'event:start_weekday', 'event:resource', 'event:duration', 'event:trace_id', 'succession:concept:name@A#B', 'succession:concept:name@B#C', 'succession:concept:name@C#D', 'succession:concept:name@D#E', 'succession:concept:name@D#F', 'succession:concept:name@E#F', 'succession:concept:name@F#process end event', 'succession:concept:name@process end event#I', 'succession:concept:name@process start event#A', 'trace:cycle_time', 'adaptation_action']\n",
      "Columns after removal:\n",
      "['trace_id', 'event:start_weekday', 'trace:cycle_time', 'adaptation_action']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "def log_pre_process(csv_file_path, memory_reduction, columns_to_drop=[]):\n",
    "    print(f'log_pre_process: {csv_file_path}')\n",
    "    data_df = load_logs(csv_file_path)\n",
    "\n",
    "    print('Columns before removal:')\n",
    "    list_of_column_names = list(data_df.columns)\n",
    "    print(list_of_column_names)\n",
    "\n",
    "    data_df = data_df.drop(columns=columns_to_drop)\n",
    "\n",
    "    print('Columns after removal:')\n",
    "    list_of_column_names = list(data_df.columns)\n",
    "    print(list_of_column_names)\n",
    "    print()\n",
    "\n",
    "    slice_start_col = 0\n",
    "    slice_end_col = len(data_df.columns)-1\n",
    "\n",
    "    values = data_df.iloc[:, slice_start_col:slice_end_col]\n",
    "    classes = data_df.iloc[:, slice_end_col:]\n",
    "\n",
    "    # enc = preprocessing.OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    # values = enc.fit_transform(values[:2])\n",
    "\n",
    "    # enc2 = preprocessing.OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    # data_df = enc2.fit_transform(data_df)\n",
    "\n",
    "    data = (values, classes)\n",
    "\n",
    "    return data, list_of_column_names[:-1], data_df\n",
    "\n",
    "\n",
    "bpic17_logs_columns_to_drop = []\n",
    "bpic17_logs_with_interventions, bpic17_logs_with_interventions_column_names, bpic17_logs_with_interventions_df = log_pre_process(bpic17_logs_with_interventions_path_encoded, memory_reduction=False, columns_to_drop=bpic17_logs_columns_to_drop)\n",
    "\n",
    "synthetic_logs_with_adaptations_path = 'sample_sequence_simulation_logs_multi_adapted_noisy_encoded.csv'\n",
    "synthetic_logs_columns_to_drop = [\"event:event_name@A\",\"event:event_name@B\",\"event:event_name@C\",\"event:event_name@D\",\"event:event_name@E\",\"event:event_name@F\",\"event:event_name@I\",\"event:event_name@process end event\",\"event:event_name@process start event\",\"event:concept:name@A\",\"event:concept:name@B\",\"event:concept:name@C\",\"event:concept:name@D\",\"event:concept:name@E\",\"event:concept:name@F\",\"event:concept:name@I\",\"event:concept:name@process end event\",\"event:concept:name@process start event\",\"event:adaptation_action@insert\",\"event:adaptation_action@no-action\",\"event:adaptation_action@skip\",\"event:@@index\",\"event:@@case_index\",\"event:trace_id\",\"succession:concept:name@A#B\",\"succession:concept:name@B#C\",\"succession:concept:name@C#D\",\"succession:concept:name@D#E\",\"succession:concept:name@D#F\",\"succession:concept:name@E#F\",\"succession:concept:name@F#process end event\",\"succession:concept:name@process end event#I\",\"succession:concept:name@process start event#A\", \"event:resource\", \"event:duration\"]\n",
    "synthetic_logs_with_adaptations, synthetic_logs_with_adaptations_column_names, synthetic_logs_with_adaptations_df = log_pre_process(synthetic_logs_with_adaptations_path, memory_reduction=False, columns_to_drop=synthetic_logs_columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Classifier comparison\n",
    "\n",
    "A comparison of a several classifiers in scikit-learn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------\n",
      "dataset: bpic17_logs_with_interventions\n",
      "------------------------------------------------------\n",
      "Classifier: Decision Tree\n",
      "10-fold cross-validation mean F1: 0.7843151284101866\n",
      "|--- duration <= 30.35\n",
      "|   |--- duration <= 12.68\n",
      "|   |   |--- CreditScore <= 898.50\n",
      "|   |   |   |--- class: 1\n",
      "|   |   |--- CreditScore >  898.50\n",
      "|   |   |   |--- CreditScore <= 899.50\n",
      "|   |   |   |   |--- class: 0\n",
      "|   |   |   |--- CreditScore >  899.50\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |--- duration >  12.68\n",
      "|   |   |--- hour <= 15.50\n",
      "|   |   |   |--- weekday <= 4.50\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |   |--- weekday >  4.50\n",
      "|   |   |   |   |--- class: 0\n",
      "|   |   |--- hour >  15.50\n",
      "|   |   |   |--- class: 0\n",
      "|--- duration >  30.35\n",
      "|   |--- CreditScore <= 898.50\n",
      "|   |   |--- class: 1\n",
      "|   |--- CreditScore >  898.50\n",
      "|   |   |--- CreditScore <= 899.50\n",
      "|   |   |   |--- duration <= 36.90\n",
      "|   |   |   |   |--- class: 0\n",
      "|   |   |   |--- duration >  36.90\n",
      "|   |   |   |   |--- class: 0\n",
      "|   |   |--- CreditScore >  899.50\n",
      "|   |   |   |--- class: 1\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "dataset: synthetic_logs_with_adaptations\n",
      "------------------------------------------------------\n",
      "Classifier: Decision Tree\n",
      "10-fold cross-validation mean F1: 0.9237324182574183\n",
      "|--- trace:cycle_time <= 250.09\n",
      "|   |--- trace:cycle_time <= 214.36\n",
      "|   |   |--- trace:cycle_time <= 206.70\n",
      "|   |   |   |--- class: no-action\n",
      "|   |   |--- trace:cycle_time >  206.70\n",
      "|   |   |   |--- class: no-action\n",
      "|   |--- trace:cycle_time >  214.36\n",
      "|   |   |--- trace:cycle_time <= 224.70\n",
      "|   |   |   |--- trace:cycle_time <= 224.44\n",
      "|   |   |   |   |--- class: no-action\n",
      "|   |   |   |--- trace:cycle_time >  224.44\n",
      "|   |   |   |   |--- class: insert\n",
      "|   |   |--- trace:cycle_time >  224.70\n",
      "|   |   |   |--- class: no-action\n",
      "|--- trace:cycle_time >  250.09\n",
      "|   |--- class: skip\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.tree import export_text\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "classifier_names = [\n",
    "    # \"Nearest Neighbors\",\n",
    "    # \"Linear SVM\",\n",
    "    # \"RBF SVM\",\n",
    "\n",
    "    # \"Gaussian Process\",\n",
    "    \"Decision Tree\",\n",
    "    # \"Random Forest\",\n",
    "    # \"Neural Net\",\n",
    "    # \"AdaBoost\",\n",
    "    # \"Naive Bayes\",\n",
    "    # \"QDA\",\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    # KNeighborsClassifier(3),\n",
    "    # SVC(kernel=\"linear\", C=0.025),\n",
    "    # SVC(gamma=2, C=1),\n",
    "\n",
    "    # GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=4, min_samples_leaf=2, ccp_alpha=0.003),\n",
    "    # RandomForestClassifier(max_depth=6, n_estimators=10, max_features=1),\n",
    "    # MLPClassifier(alpha=1, max_iter=1000),\n",
    "    # AdaBoostClassifier(),\n",
    "    # GaussianNB(),\n",
    "    # QuadraticDiscriminantAnalysis(),\n",
    "]\n",
    "\n",
    "datasets_df = [\n",
    "    bpic17_logs_with_interventions_df,\n",
    "    synthetic_logs_with_adaptations_df\n",
    "]\n",
    "\n",
    "datasets = [\n",
    "    bpic17_logs_with_interventions,\n",
    "    synthetic_logs_with_adaptations\n",
    "]\n",
    "\n",
    "dataset_names = [\n",
    "    \"bpic17_logs_with_interventions\",\n",
    "    \"synthetic_logs_with_adaptations\"\n",
    "]\n",
    "\n",
    "dataset_feature_names = [\n",
    "    bpic17_logs_with_interventions_column_names,\n",
    "    synthetic_logs_with_adaptations_column_names\n",
    "]\n",
    "\n",
    "# iterate over datasets\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    \n",
    "    X, y = ds\n",
    "\n",
    "    print('---------------------------------------------------------------------')\n",
    "    print(f'dataset: {dataset_names[ds_cnt]}')\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for classifier_name, classifier in zip(classifier_names, classifiers):\n",
    "\n",
    "        print('------------------------------------------------------')\n",
    "        print('Classifier:', classifier_name)\n",
    "\n",
    "        scores = cross_val_score(classifier, X, y.values.ravel(), cv=10, scoring='f1_weighted')\n",
    "        \n",
    "        print('10-fold cross-validation mean F1:', numpy.mean(scores))\n",
    "\n",
    "        if classifier_name == 'Decision Tree':\n",
    "            classifier.fit(X, y.values.ravel())\n",
    "            text_representation = tree.export_text(classifier, feature_names=dataset_feature_names[ds_cnt])\n",
    "            print(text_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi train / test\n",
      "\n",
      " dataset: bpic17_logs_with_interventions\n",
      "train percentage= 20\n",
      "f1 score= 0.7952862062657106\n",
      "train percentage= 40\n",
      "f1 score= 0.7912566036707415\n",
      "train percentage= 66\n",
      "f1 score= 0.7781674538969476\n",
      "\n",
      " dataset: synthetic_logs_with_adaptations\n",
      "train percentage= 20\n",
      "f1 score= 0.8971681415929202\n",
      "train percentage= 40\n",
      "f1 score= 0.9351336055619529\n",
      "train percentage= 66\n",
      "f1 score= 0.9288397552860364\n"
     ]
    }
   ],
   "source": [
    "models_folder = './models/'\n",
    "\n",
    "def store_classifier(classifier, file_path):\n",
    "    from joblib import dump\n",
    "    dump(classifier, models_folder + file_path)\n",
    "\n",
    "def load_classifier(file_path):\n",
    "    from joblib import load\n",
    "    classifier = load(models_folder + file_path)\n",
    "    return classifier\n",
    "\n",
    "def multi_train_test(train_percentages, test_train_ratio):\n",
    "    print('Multi train / test')\n",
    "    classifier = DecisionTreeClassifier(max_depth=4, min_samples_leaf=2, ccp_alpha=0.003)\n",
    "\n",
    "    for ds_cnt, dataset in enumerate(datasets):\n",
    "        print(f'\\n dataset: {dataset_names[ds_cnt]}')\n",
    "        values, classes = dataset\n",
    "        # print(values)\n",
    "        # print(classes)\n",
    "        dataset_length = len(dataset[0])\n",
    "\n",
    "        for train_percentage in train_percentages:\n",
    "            print(f'train percentage= {train_percentage}')\n",
    "            split_point_train = int(train_percentage / 100 * dataset_length)\n",
    "            split_point_test = int(split_point_train + test_train_ratio * split_point_train)\n",
    "\n",
    "            train_set_values = values[:split_point_train]\n",
    "            train_set_classes = classes[:split_point_train]\n",
    "\n",
    "            test_set_values = values[split_point_train:split_point_test]\n",
    "            test_set_classes = classes[split_point_train:split_point_test]\n",
    "\n",
    "            classifier.fit(train_set_values, train_set_classes)\n",
    "            y_pred = classifier.predict(test_set_values)\n",
    "            f1 = f1_score(y_true = test_set_classes, y_pred = y_pred, average = 'weighted')\n",
    "            print(f'f1 score= {f1}')\n",
    "            \n",
    "            classifier_file_path = dataset_names[ds_cnt] + '_' + str(train_percentage) + '_percent' + '.joblib'\n",
    "            store_classifier(classifier, classifier_file_path)\n",
    "\n",
    "            train_logs_path = dataset_names[ds_cnt] + '_' + str(train_percentage) + '_percent_train' + '.csv'\n",
    "            store_logs(datasets_df[ds_cnt][:split_point_train], train_logs_path)\n",
    "\n",
    "            test_logs_path = dataset_names[ds_cnt] + '_' + str(train_percentage) + '_percent_test' + '.csv'\n",
    "            store_logs(datasets_df[ds_cnt][split_point_train:split_point_test], test_logs_path)\n",
    "\n",
    "\n",
    "train_percentages = [20, 40, 66]\n",
    "test_train_ratio = 0.5\n",
    "multi_train_test(train_percentages, test_train_ratio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "corrid-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "34ff90f5239f9160625527e13c748f95b5880a9ad27056497ef1c46bfe84f008"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
